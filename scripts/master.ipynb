{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac16421b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import logging\n",
    "import time\n",
    "import gc\n",
    "import threading\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from typing import Dict, List, Any, Tuple\n",
    "from collections import defaultdict\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "COLD_STORAGE_DIR = \"cold_storage\"\n",
    "OUTPUT_DIR = \"master_parquet_files\"\n",
    "BATCH_SIZE = 50000\n",
    "\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('options_master_parquet_generator.log'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "df22b704",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ensure_output_directory():\n",
    "    \"\"\"Create output directory if it doesn't exist.\"\"\"\n",
    "    if not os.path.exists(OUTPUT_DIR):\n",
    "        os.makedirs(OUTPUT_DIR)\n",
    "        logger.info(f\"Created output directory: {OUTPUT_DIR}\")\n",
    "    else:\n",
    "        logger.info(f\"Using existing output directory: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "29c30cef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_file_size(file_path):\n",
    "    \"\"\"Get file size in bytes.\"\"\"\n",
    "    return os.path.getsize(file_path)\n",
    "\n",
    "def format_size(size_bytes):\n",
    "    \"\"\"Format bytes into human readable format.\"\"\"\n",
    "    for unit in ['B', 'KB', 'MB', 'GB']:\n",
    "        if size_bytes < 1024.0:\n",
    "            return f\"{size_bytes:.2f} {unit}\"\n",
    "        size_bytes /= 1024.0\n",
    "    return f\"{size_bytes:.2f} TB\"\n",
    "\n",
    "def standardize_columns(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Standardize column names and ensure required columns exist.\"\"\"\n",
    "    column_mapping = {\n",
    "        'o': 'open',\n",
    "        'h': 'high', \n",
    "        'l': 'low',\n",
    "        'c': 'close',\n",
    "        'v': 'volume',\n",
    "        'oi': 'open_interest'\n",
    "    }\n",
    "    \n",
    "    df = df.rename(columns=column_mapping)\n",
    "    \n",
    "    required_columns = ['timestamp', 'open', 'high', 'low', 'close', 'volume', 'open_interest']\n",
    "    for col in required_columns:\n",
    "        if col not in df.columns:\n",
    "            if col == 'timestamp':\n",
    "                logger.error(f\"Missing critical timestamp column in dataframe\")\n",
    "                raise ValueError(\"Timestamp column is required\")\n",
    "            else:\n",
    "                df[col] = None\n",
    "                logger.warning(f\"Added missing column '{col}' with NULL values\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "def generate_option_symbol(underlying: str, expiry_str: str, strike: int, option_type: str) -> str:\n",
    "    \"\"\"Generate standardized option symbol.\"\"\"\n",
    "    try:\n",
    "        exp_date = datetime.strptime(expiry_str, '%Y%m%d')\n",
    "        symbol_suffix = 'CE' if option_type.upper() == 'CE' or option_type.lower() == 'call' else 'PE'\n",
    "        return f\"{underlying}{exp_date.strftime('%y%b').upper()}{strike}{symbol_suffix}\"\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error generating symbol for {underlying}-{expiry_str}-{strike}-{option_type}: {e}\")\n",
    "        return f\"{underlying}_{expiry_str}_{strike}_{option_type}\"\n",
    "\n",
    "def parse_option_file_info(file_path: str, underlying: str) -> Tuple[str, str, int, str]:\n",
    "    \"\"\"Parse option file path to extract expiry, strike, and option type.\"\"\"\n",
    "    path_parts = file_path.split(os.sep)\n",
    "    \n",
    "    expiry = None\n",
    "    strike = None\n",
    "    option_type = None\n",
    "    \n",
    "    for part in path_parts:\n",
    "        if len(part) == 8 and part.isdigit():\n",
    "            expiry = part\n",
    "        elif part.isdigit() and len(part) <= 6:\n",
    "            strike = int(part)\n",
    "    \n",
    "    filename = os.path.basename(file_path)\n",
    "    if 'CE' in filename.upper():\n",
    "        option_type = 'call'\n",
    "    elif 'PE' in filename.upper():\n",
    "        option_type = 'put'\n",
    "    elif '_call' in filename.lower():\n",
    "        option_type = 'call'\n",
    "    elif '_put' in filename.lower():\n",
    "        option_type = 'put'\n",
    "    \n",
    "    if not all([expiry, strike is not None, option_type]):\n",
    "        logger.warning(f\"Could not fully parse file info from {file_path}\")\n",
    "        if not expiry:\n",
    "            expiry = \"00000000\"\n",
    "        if strike is None:\n",
    "            strike = 0\n",
    "        if not option_type:\n",
    "            option_type = \"unknown\"\n",
    "    \n",
    "    return expiry, str(strike), strike, option_type\n",
    "\n",
    "def collect_option_files(underlying: str) -> List[Dict[str, Any]]:\n",
    "    \"\"\"Collect all parquet files for a specific underlying.\"\"\"\n",
    "    logger.info(f\"Collecting option files for {underlying}\")\n",
    "    \n",
    "    underlying_path = os.path.join(COLD_STORAGE_DIR, \"BSE\", \"Options\", underlying)\n",
    "    \n",
    "    if not os.path.exists(underlying_path):\n",
    "        logger.warning(f\"Path does not exist: {underlying_path}\")\n",
    "        return []\n",
    "    \n",
    "    option_files = []\n",
    "    total_size = 0\n",
    "    \n",
    "    for root, dirs, files in os.walk(underlying_path):\n",
    "        for file in files:\n",
    "            if file.endswith('.parquet'):\n",
    "                file_path = os.path.join(root, file)\n",
    "                file_size = get_file_size(file_path)\n",
    "                total_size += file_size\n",
    "                \n",
    "                expiry, strike_str, strike_int, option_type = parse_option_file_info(file_path, underlying)\n",
    "                \n",
    "                option_files.append({\n",
    "                    'file_path': file_path,\n",
    "                    'expiry': expiry,\n",
    "                    'strike_str': strike_str,\n",
    "                    'strike_int': strike_int,\n",
    "                    'option_type': option_type,\n",
    "                    'file_size': file_size,\n",
    "                    'relative_path': os.path.relpath(file_path, COLD_STORAGE_DIR)\n",
    "                })\n",
    "    \n",
    "    logger.info(f\"Found {len(option_files)} option files for {underlying} \"\n",
    "               f\"(Total size: {format_size(total_size)})\")\n",
    "    \n",
    "    return option_files\n",
    "\n",
    "def process_option_file(file_info: Dict[str, Any], underlying: str) -> pd.DataFrame:\n",
    "    \"\"\"Process a single option parquet file and return standardized DataFrame.\"\"\"\n",
    "    file_path = file_info['file_path']\n",
    "    \n",
    "    try:\n",
    "        df = pd.read_parquet(file_path)\n",
    "        \n",
    "        if df.empty:\n",
    "            logger.warning(f\"Empty file: {file_path}\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        df = standardize_columns(df)\n",
    "        \n",
    "        df['symbol'] = generate_option_symbol(\n",
    "            underlying, \n",
    "            file_info['expiry'], \n",
    "            file_info['strike_int'], \n",
    "            file_info['option_type']\n",
    "        )\n",
    "        df['underlying'] = underlying\n",
    "        df['expiry'] = pd.to_datetime(file_info['expiry'], format='%Y%m%d').date()\n",
    "        df['strike'] = file_info['strike_int']\n",
    "        df['option_type'] = file_info['option_type']\n",
    "        \n",
    "        if 'timestamp' in df.columns:\n",
    "            df['timestamp'] = pd.to_datetime(df['timestamp'])\n",
    "        \n",
    "        column_order = [\n",
    "            'timestamp', 'symbol', 'underlying', 'expiry', 'strike', 'option_type',\n",
    "            'open', 'high', 'low', 'close', 'volume', 'open_interest'\n",
    "        ]\n",
    "        \n",
    "        available_columns = [col for col in column_order if col in df.columns]\n",
    "        df = df[available_columns]\n",
    "        \n",
    "        logger.debug(f\"Processed {file_path}: {len(df)} rows\")\n",
    "        return df\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Error processing file {file_path}: {e}\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "def create_master_parquet_for_underlying(underlying: str) -> bool:\n",
    "    \"\"\"Create master parquet file for a specific underlying.\"\"\"\n",
    "    logger.info(f\"Creating master parquet file for {underlying}\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    try:\n",
    "        option_files = collect_option_files(underlying)\n",
    "        \n",
    "        if not option_files:\n",
    "            logger.warning(f\"No option files found for {underlying}\")\n",
    "            return False\n",
    "        \n",
    "        option_files.sort(key=lambda x: (x['expiry'], x['strike_int'], x['option_type']))\n",
    "        \n",
    "        all_dataframes = []\n",
    "        processed_files = 0\n",
    "        failed_files = 0\n",
    "        total_rows = 0\n",
    "        \n",
    "        logger.info(f\"Processing {len(option_files)} files for {underlying}\")\n",
    "        \n",
    "        for i, file_info in enumerate(option_files):\n",
    "            df = process_option_file(file_info, underlying)\n",
    "            \n",
    "            if not df.empty:\n",
    "                all_dataframes.append(df)\n",
    "                total_rows += len(df)\n",
    "                processed_files += 1\n",
    "            else:\n",
    "                failed_files += 1\n",
    "            \n",
    "            if (i + 1) % 100 == 0:\n",
    "                logger.info(f\"Processed {i + 1}/{len(option_files)} files for {underlying}\")\n",
    "                \n",
    "                if len(all_dataframes) >= 50:\n",
    "                    combined_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "                    all_dataframes = [combined_df]\n",
    "                    gc.collect()\n",
    "        \n",
    "        if not all_dataframes:\n",
    "            logger.warning(f\"No valid data found for {underlying}\")\n",
    "            return False\n",
    "        \n",
    "        logger.info(f\"Combining {len(all_dataframes)} dataframe chunks for {underlying}\")\n",
    "        master_df = pd.concat(all_dataframes, ignore_index=True)\n",
    "        \n",
    "        logger.info(f\"Sorting {len(master_df)} rows by timestamp for {underlying}\")\n",
    "        master_df = master_df.sort_values('timestamp').reset_index(drop=True)\n",
    "        \n",
    "        output_file = os.path.join(OUTPUT_DIR, f\"{underlying.lower()}_options_master.parquet\")\n",
    "        logger.info(f\"Saving master parquet file: {output_file}\")\n",
    "        \n",
    "        table = pa.Table.from_pandas(master_df)\n",
    "        pq.write_table(\n",
    "            table, \n",
    "            output_file,\n",
    "            compression='snappy',\n",
    "            use_dictionary=True,\n",
    "            row_group_size=50000\n",
    "        )\n",
    "\n",
    "        output_size = get_file_size(output_file)\n",
    "        \n",
    "        elapsed_time = time.time() - start_time\n",
    "        \n",
    "        logger.info(f\"✅ Successfully created master parquet for {underlying}:\")\n",
    "        logger.info(f\"   - Output file: {output_file}\")\n",
    "        logger.info(f\"   - Total rows: {len(master_df):,}\")\n",
    "        logger.info(f\"   - Processed files: {processed_files}/{len(option_files)}\")\n",
    "        logger.info(f\"   - Failed files: {failed_files}\")\n",
    "        logger.info(f\"   - Output size: {format_size(output_size)}\")\n",
    "        logger.info(f\"   - Processing time: {elapsed_time:.2f} seconds\")\n",
    "        logger.info(f\"   - Processing rate: {len(master_df) / elapsed_time:.0f} rows/second\")\n",
    "        \n",
    "        if 'timestamp' in master_df.columns and not master_df['timestamp'].isna().all():\n",
    "            min_date = master_df['timestamp'].min()\n",
    "            max_date = master_df['timestamp'].max()\n",
    "            logger.info(f\"   - Date range: {min_date} to {max_date}\")\n",
    "        \n",
    "        unique_symbols = master_df['symbol'].nunique() if 'symbol' in master_df.columns else 0\n",
    "        unique_expiries = master_df['expiry'].nunique() if 'expiry' in master_df.columns else 0\n",
    "        unique_strikes = master_df['strike'].nunique() if 'strike' in master_df.columns else 0\n",
    "        \n",
    "        logger.info(f\"   - Unique symbols: {unique_symbols}\")\n",
    "        logger.info(f\"   - Unique expiries: {unique_expiries}\")\n",
    "        logger.info(f\"   - Unique strikes: {unique_strikes}\")\n",
    "        \n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        elapsed_time = time.time() - start_time\n",
    "        logger.error(f\"❌ Failed to create master parquet for {underlying} \"\n",
    "                    f\"after {elapsed_time:.2f} seconds: {e}\")\n",
    "        return False\n",
    "    finally:\n",
    "        gc.collect()\n",
    "\n",
    "def find_available_underlyings() -> List[str]:\n",
    "    \"\"\"Find all available underlyings in the cold storage directory.\"\"\"\n",
    "    logger.info(\"Scanning for available underlyings in cold storage\")\n",
    "    \n",
    "    options_path = os.path.join(COLD_STORAGE_DIR, \"BSE\", \"Options\")\n",
    "    \n",
    "    if not os.path.exists(options_path):\n",
    "        logger.error(f\"Options path does not exist: {options_path}\")\n",
    "        return []\n",
    "    \n",
    "    underlyings = []\n",
    "    for item in os.listdir(options_path):\n",
    "        item_path = os.path.join(options_path, item)\n",
    "        if os.path.isdir(item_path):\n",
    "            underlyings.append(item)\n",
    "    \n",
    "    logger.info(f\"Found {len(underlyings)} underlyings: {underlyings}\")\n",
    "    return underlyings\n",
    "\n",
    "def generate_final_summary():\n",
    "    \"\"\"Generate final summary of created master parquet files.\"\"\"\n",
    "    logger.info(\"Generating final summary of created master parquet files\")\n",
    "    \n",
    "    if not os.path.exists(OUTPUT_DIR):\n",
    "        logger.warning(\"Output directory does not exist\")\n",
    "        return\n",
    "    \n",
    "    master_files = [f for f in os.listdir(OUTPUT_DIR) if f.endswith('_options_master.parquet')]\n",
    "    \n",
    "    if not master_files:\n",
    "        logger.warning(\"No master parquet files found\")\n",
    "        return\n",
    "    \n",
    "    logger.info(f\"Summary of {len(master_files)} created master parquet files:\")\n",
    "    \n",
    "    total_size = 0\n",
    "    for file in sorted(master_files):\n",
    "        file_path = os.path.join(OUTPUT_DIR, file)\n",
    "        file_size = get_file_size(file_path)\n",
    "        total_size += file_size\n",
    "        \n",
    "        underlying = file.replace('_options_master.parquet', '').upper()\n",
    "        \n",
    "        try:\n",
    "            parquet_file = pq.ParquetFile(file_path)\n",
    "            row_count = parquet_file.metadata.num_rows\n",
    "            logger.info(f\"  - {underlying}: {row_count:,} rows ({format_size(file_size)})\")\n",
    "        except Exception as e:\n",
    "            logger.info(f\"  - {underlying}: {format_size(file_size)} (row count unavailable)\")\n",
    "    \n",
    "    logger.info(f\"Total size of all master files: {format_size(total_size)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3dd39fec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-05-30 14:27:36,191 - INFO - ======================================================================\n",
      "2025-05-30 14:27:36,193 - INFO - Starting Options Master Parquet File Generation\n",
      "2025-05-30 14:27:36,195 - INFO - ======================================================================\n",
      "2025-05-30 14:27:36,196 - INFO - Using existing output directory: master_parquet_files\n",
      "2025-05-30 14:27:36,198 - INFO - Scanning for available underlyings in cold storage\n",
      "2025-05-30 14:27:36,199 - INFO - Found 2 underlyings: ['BANKEX', 'SENSEX']\n",
      "2025-05-30 14:27:36,200 - INFO - Starting processing for underlying: BANKEX\n",
      "2025-05-30 14:27:36,201 - INFO - Creating master parquet file for BANKEX\n",
      "2025-05-30 14:27:36,202 - INFO - Collecting option files for BANKEX\n",
      "2025-05-30 14:27:36,265 - INFO - Found 1599 option files for BANKEX (Total size: 29.25 MB)\n",
      "2025-05-30 14:27:36,266 - INFO - Processing 1599 files for BANKEX\n",
      "2025-05-30 14:27:37,148 - INFO - Processed 100/1599 files for BANKEX\n",
      "2025-05-30 14:27:38,275 - INFO - Processed 200/1599 files for BANKEX\n",
      "2025-05-30 14:27:39,125 - INFO - Processed 300/1599 files for BANKEX\n",
      "2025-05-30 14:27:40,014 - INFO - Processed 400/1599 files for BANKEX\n",
      "2025-05-30 14:27:40,772 - INFO - Processed 500/1599 files for BANKEX\n",
      "2025-05-30 14:27:42,056 - INFO - Processed 600/1599 files for BANKEX\n",
      "2025-05-30 14:27:43,176 - INFO - Processed 700/1599 files for BANKEX\n",
      "2025-05-30 14:27:44,058 - INFO - Processed 800/1599 files for BANKEX\n",
      "2025-05-30 14:27:44,926 - INFO - Processed 900/1599 files for BANKEX\n",
      "2025-05-30 14:27:45,677 - INFO - Processed 1000/1599 files for BANKEX\n",
      "2025-05-30 14:27:46,519 - INFO - Processed 1100/1599 files for BANKEX\n",
      "2025-05-30 14:27:47,467 - INFO - Processed 1200/1599 files for BANKEX\n",
      "2025-05-30 14:27:48,238 - INFO - Processed 1300/1599 files for BANKEX\n",
      "2025-05-30 14:27:48,933 - INFO - Processed 1400/1599 files for BANKEX\n",
      "2025-05-30 14:27:49,763 - INFO - Processed 1500/1599 files for BANKEX\n",
      "2025-05-30 14:27:50,575 - INFO - Combining 100 dataframe chunks for BANKEX\n",
      "2025-05-30 14:27:50,611 - INFO - Sorting 1061116 rows by timestamp for BANKEX\n",
      "2025-05-30 14:27:51,269 - INFO - Saving master parquet file: master_parquet_files/bankex_options_master.parquet\n",
      "2025-05-30 14:27:52,210 - INFO - ✅ Successfully created master parquet for BANKEX:\n",
      "2025-05-30 14:27:52,212 - INFO -    - Output file: master_parquet_files/bankex_options_master.parquet\n",
      "2025-05-30 14:27:52,213 - INFO -    - Total rows: 1,061,116\n",
      "2025-05-30 14:27:52,214 - INFO -    - Processed files: 1599/1599\n",
      "2025-05-30 14:27:52,215 - INFO -    - Failed files: 0\n",
      "2025-05-30 14:27:52,216 - INFO -    - Output size: 15.69 MB\n",
      "2025-05-30 14:27:52,217 - INFO -    - Processing time: 16.01 seconds\n",
      "2025-05-30 14:27:52,218 - INFO -    - Processing rate: 66287 rows/second\n",
      "2025-05-30 14:27:52,226 - INFO -    - Date range: 2024-12-02 09:15:00 to 2025-04-09 15:30:00\n",
      "2025-05-30 14:27:52,299 - INFO -    - Unique symbols: 1360\n",
      "2025-05-30 14:27:52,300 - INFO -    - Unique expiries: 7\n",
      "2025-05-30 14:27:52,301 - INFO -    - Unique strikes: 156\n",
      "2025-05-30 14:27:52,363 - INFO - ✅ Successfully completed BANKEX\n",
      "2025-05-30 14:27:52,366 - INFO - Starting processing for underlying: SENSEX\n",
      "2025-05-30 14:27:52,367 - INFO - Creating master parquet file for SENSEX\n",
      "2025-05-30 14:27:52,368 - INFO - Collecting option files for SENSEX\n",
      "2025-05-30 14:27:53,030 - INFO - Found 16747 option files for SENSEX (Total size: 816.95 MB)\n",
      "2025-05-30 14:27:53,042 - INFO - Processing 16747 files for SENSEX\n",
      "2025-05-30 14:27:53,712 - INFO - Processed 100/16747 files for SENSEX\n",
      "2025-05-30 14:27:54,628 - INFO - Processed 200/16747 files for SENSEX\n",
      "2025-05-30 14:27:55,595 - INFO - Processed 300/16747 files for SENSEX\n",
      "2025-05-30 14:27:56,438 - INFO - Processed 400/16747 files for SENSEX\n",
      "2025-05-30 14:27:57,289 - INFO - Processed 500/16747 files for SENSEX\n",
      "2025-05-30 14:27:58,197 - INFO - Processed 600/16747 files for SENSEX\n",
      "2025-05-30 14:27:59,167 - INFO - Processed 700/16747 files for SENSEX\n",
      "2025-05-30 14:28:00,038 - INFO - Processed 800/16747 files for SENSEX\n",
      "2025-05-30 14:28:00,857 - INFO - Processed 900/16747 files for SENSEX\n",
      "2025-05-30 14:28:01,827 - INFO - Processed 1000/16747 files for SENSEX\n",
      "2025-05-30 14:28:02,882 - INFO - Processed 1100/16747 files for SENSEX\n",
      "2025-05-30 14:28:03,832 - INFO - Processed 1200/16747 files for SENSEX\n",
      "2025-05-30 14:28:04,621 - INFO - Processed 1300/16747 files for SENSEX\n",
      "2025-05-30 14:28:05,587 - INFO - Processed 1400/16747 files for SENSEX\n",
      "2025-05-30 14:28:06,488 - INFO - Processed 1500/16747 files for SENSEX\n",
      "2025-05-30 14:28:07,422 - INFO - Processed 1600/16747 files for SENSEX\n",
      "2025-05-30 14:28:08,585 - INFO - Processed 1700/16747 files for SENSEX\n",
      "2025-05-30 14:28:09,919 - INFO - Processed 1800/16747 files for SENSEX\n",
      "2025-05-30 14:28:10,975 - INFO - Processed 1900/16747 files for SENSEX\n",
      "2025-05-30 14:28:12,110 - INFO - Processed 2000/16747 files for SENSEX\n",
      "2025-05-30 14:28:13,324 - INFO - Processed 2100/16747 files for SENSEX\n",
      "2025-05-30 14:28:14,347 - INFO - Processed 2200/16747 files for SENSEX\n",
      "2025-05-30 14:28:15,319 - INFO - Processed 2300/16747 files for SENSEX\n",
      "2025-05-30 14:28:16,545 - INFO - Processed 2400/16747 files for SENSEX\n",
      "2025-05-30 14:28:17,806 - INFO - Processed 2500/16747 files for SENSEX\n",
      "2025-05-30 14:28:18,867 - INFO - Processed 2600/16747 files for SENSEX\n",
      "2025-05-30 14:28:20,317 - INFO - Processed 2700/16747 files for SENSEX\n",
      "2025-05-30 14:28:21,335 - INFO - Processed 2800/16747 files for SENSEX\n",
      "2025-05-30 14:28:22,351 - INFO - Processed 2900/16747 files for SENSEX\n",
      "2025-05-30 14:28:23,709 - INFO - Processed 3000/16747 files for SENSEX\n",
      "2025-05-30 14:28:25,090 - INFO - Processed 3100/16747 files for SENSEX\n",
      "2025-05-30 14:28:26,356 - INFO - Processed 3200/16747 files for SENSEX\n",
      "2025-05-30 14:28:27,899 - INFO - Processed 3300/16747 files for SENSEX\n",
      "2025-05-30 14:28:29,222 - INFO - Processed 3400/16747 files for SENSEX\n",
      "2025-05-30 14:28:30,476 - INFO - Processed 3500/16747 files for SENSEX\n",
      "2025-05-30 14:28:31,884 - INFO - Processed 3600/16747 files for SENSEX\n",
      "2025-05-30 14:28:33,216 - INFO - Processed 3700/16747 files for SENSEX\n",
      "2025-05-30 14:28:34,388 - INFO - Processed 3800/16747 files for SENSEX\n",
      "2025-05-30 14:28:35,809 - INFO - Processed 3900/16747 files for SENSEX\n",
      "2025-05-30 14:28:37,240 - INFO - Processed 4000/16747 files for SENSEX\n",
      "2025-05-30 14:28:38,273 - INFO - Processed 4100/16747 files for SENSEX\n",
      "2025-05-30 14:28:39,746 - INFO - Processed 4200/16747 files for SENSEX\n",
      "2025-05-30 14:28:41,337 - INFO - Processed 4300/16747 files for SENSEX\n",
      "2025-05-30 14:28:42,888 - INFO - Processed 4400/16747 files for SENSEX\n",
      "2025-05-30 14:28:44,299 - INFO - Processed 4500/16747 files for SENSEX\n",
      "2025-05-30 14:28:45,815 - INFO - Processed 4600/16747 files for SENSEX\n",
      "2025-05-30 14:28:47,484 - INFO - Processed 4700/16747 files for SENSEX\n",
      "2025-05-30 14:28:49,348 - INFO - Processed 4800/16747 files for SENSEX\n",
      "2025-05-30 14:28:51,189 - INFO - Processed 4900/16747 files for SENSEX\n",
      "2025-05-30 14:28:52,878 - INFO - Processed 5000/16747 files for SENSEX\n",
      "2025-05-30 14:28:54,761 - INFO - Processed 5100/16747 files for SENSEX\n",
      "2025-05-30 14:28:56,336 - INFO - Processed 5200/16747 files for SENSEX\n",
      "2025-05-30 14:28:58,321 - INFO - Processed 5300/16747 files for SENSEX\n",
      "2025-05-30 14:28:59,696 - INFO - Processed 5400/16747 files for SENSEX\n",
      "2025-05-30 14:29:01,215 - INFO - Processed 5500/16747 files for SENSEX\n",
      "2025-05-30 14:29:02,468 - INFO - Processed 5600/16747 files for SENSEX\n",
      "2025-05-30 14:29:03,935 - INFO - Processed 5700/16747 files for SENSEX\n",
      "2025-05-30 14:29:05,815 - INFO - Processed 5800/16747 files for SENSEX\n",
      "2025-05-30 14:29:07,212 - INFO - Processed 5900/16747 files for SENSEX\n",
      "2025-05-30 14:29:09,277 - INFO - Processed 6000/16747 files for SENSEX\n",
      "2025-05-30 14:29:10,794 - INFO - Processed 6100/16747 files for SENSEX\n",
      "2025-05-30 14:29:12,557 - INFO - Processed 6200/16747 files for SENSEX\n",
      "2025-05-30 14:29:14,118 - INFO - Processed 6300/16747 files for SENSEX\n",
      "2025-05-30 14:29:16,216 - INFO - Processed 6400/16747 files for SENSEX\n",
      "2025-05-30 14:29:18,063 - INFO - Processed 6500/16747 files for SENSEX\n",
      "2025-05-30 14:29:19,514 - INFO - Processed 6600/16747 files for SENSEX\n",
      "2025-05-30 14:29:21,166 - INFO - Processed 6700/16747 files for SENSEX\n",
      "2025-05-30 14:29:22,505 - INFO - Processed 6800/16747 files for SENSEX\n",
      "2025-05-30 14:29:24,187 - INFO - Processed 6900/16747 files for SENSEX\n",
      "2025-05-30 14:29:25,735 - INFO - Processed 7000/16747 files for SENSEX\n",
      "2025-05-30 14:29:27,812 - INFO - Processed 7100/16747 files for SENSEX\n",
      "2025-05-30 14:29:29,268 - INFO - Processed 7200/16747 files for SENSEX\n",
      "2025-05-30 14:29:31,253 - INFO - Processed 7300/16747 files for SENSEX\n",
      "2025-05-30 14:29:32,524 - INFO - Processed 7400/16747 files for SENSEX\n",
      "2025-05-30 14:29:34,489 - INFO - Processed 7500/16747 files for SENSEX\n",
      "2025-05-30 14:29:36,297 - INFO - Processed 7600/16747 files for SENSEX\n",
      "2025-05-30 14:29:38,359 - INFO - Processed 7700/16747 files for SENSEX\n",
      "2025-05-30 14:29:40,700 - INFO - Processed 7800/16747 files for SENSEX\n",
      "2025-05-30 14:29:43,205 - INFO - Processed 7900/16747 files for SENSEX\n",
      "2025-05-30 14:29:45,872 - INFO - Processed 8000/16747 files for SENSEX\n",
      "2025-05-30 14:29:48,295 - INFO - Processed 8100/16747 files for SENSEX\n",
      "2025-05-30 14:29:50,806 - INFO - Processed 8200/16747 files for SENSEX\n",
      "2025-05-30 14:29:54,050 - INFO - Processed 8300/16747 files for SENSEX\n",
      "2025-05-30 14:29:56,778 - INFO - Processed 8400/16747 files for SENSEX\n",
      "2025-05-30 14:29:59,605 - INFO - Processed 8500/16747 files for SENSEX\n",
      "2025-05-30 14:30:02,748 - INFO - Processed 8600/16747 files for SENSEX\n",
      "2025-05-30 14:30:05,376 - INFO - Processed 8700/16747 files for SENSEX\n",
      "2025-05-30 14:30:07,400 - INFO - Processed 8800/16747 files for SENSEX\n",
      "2025-05-30 14:30:10,415 - INFO - Processed 8900/16747 files for SENSEX\n",
      "2025-05-30 14:30:13,305 - INFO - Processed 9000/16747 files for SENSEX\n",
      "2025-05-30 14:30:16,282 - INFO - Processed 9100/16747 files for SENSEX\n",
      "2025-05-30 14:30:18,703 - INFO - Processed 9200/16747 files for SENSEX\n",
      "2025-05-30 14:30:20,904 - INFO - Processed 9300/16747 files for SENSEX\n",
      "2025-05-30 14:30:22,971 - INFO - Processed 9400/16747 files for SENSEX\n",
      "2025-05-30 14:30:25,910 - INFO - Processed 9500/16747 files for SENSEX\n",
      "2025-05-30 14:30:28,157 - INFO - Processed 9600/16747 files for SENSEX\n",
      "2025-05-30 14:30:31,551 - INFO - Processed 9700/16747 files for SENSEX\n",
      "2025-05-30 14:30:33,991 - INFO - Processed 9800/16747 files for SENSEX\n",
      "2025-05-30 14:30:37,450 - INFO - Processed 9900/16747 files for SENSEX\n",
      "2025-05-30 14:30:41,120 - INFO - Processed 10000/16747 files for SENSEX\n",
      "2025-05-30 14:30:44,245 - INFO - Processed 10100/16747 files for SENSEX\n",
      "2025-05-30 14:30:47,275 - INFO - Processed 10200/16747 files for SENSEX\n",
      "2025-05-30 14:30:50,774 - INFO - Processed 10300/16747 files for SENSEX\n",
      "2025-05-30 14:30:53,764 - INFO - Processed 10400/16747 files for SENSEX\n",
      "2025-05-30 14:30:57,826 - INFO - Processed 10500/16747 files for SENSEX\n",
      "2025-05-30 14:31:00,764 - INFO - Processed 10600/16747 files for SENSEX\n",
      "2025-05-30 14:31:04,133 - INFO - Processed 10700/16747 files for SENSEX\n",
      "2025-05-30 14:31:07,484 - INFO - Processed 10800/16747 files for SENSEX\n",
      "2025-05-30 14:31:10,290 - INFO - Processed 10900/16747 files for SENSEX\n",
      "2025-05-30 14:31:13,652 - INFO - Processed 11000/16747 files for SENSEX\n",
      "2025-05-30 14:31:16,581 - INFO - Processed 11100/16747 files for SENSEX\n",
      "2025-05-30 14:31:19,876 - INFO - Processed 11200/16747 files for SENSEX\n",
      "2025-05-30 14:31:23,272 - INFO - Processed 11300/16747 files for SENSEX\n",
      "2025-05-30 14:31:26,019 - INFO - Processed 11400/16747 files for SENSEX\n",
      "2025-05-30 14:31:29,391 - INFO - Processed 11500/16747 files for SENSEX\n",
      "2025-05-30 14:31:32,744 - INFO - Processed 11600/16747 files for SENSEX\n",
      "2025-05-30 14:31:35,581 - INFO - Processed 11700/16747 files for SENSEX\n",
      "2025-05-30 14:31:38,558 - INFO - Processed 11800/16747 files for SENSEX\n",
      "2025-05-30 14:31:41,769 - INFO - Processed 11900/16747 files for SENSEX\n",
      "2025-05-30 14:31:44,529 - INFO - Processed 12000/16747 files for SENSEX\n",
      "2025-05-30 14:31:46,903 - INFO - Processed 12100/16747 files for SENSEX\n",
      "2025-05-30 14:31:49,164 - INFO - Processed 12200/16747 files for SENSEX\n",
      "2025-05-30 14:31:53,310 - INFO - Processed 12300/16747 files for SENSEX\n",
      "2025-05-30 14:31:56,203 - INFO - Processed 12400/16747 files for SENSEX\n",
      "2025-05-30 14:31:59,214 - INFO - Processed 12500/16747 files for SENSEX\n",
      "2025-05-30 14:32:02,150 - INFO - Processed 12600/16747 files for SENSEX\n",
      "2025-05-30 14:32:05,453 - INFO - Processed 12700/16747 files for SENSEX\n",
      "2025-05-30 14:32:08,219 - INFO - Processed 12800/16747 files for SENSEX\n",
      "2025-05-30 14:32:10,999 - INFO - Processed 12900/16747 files for SENSEX\n",
      "2025-05-30 14:32:14,467 - INFO - Processed 13000/16747 files for SENSEX\n",
      "2025-05-30 14:32:17,493 - INFO - Processed 13100/16747 files for SENSEX\n",
      "2025-05-30 14:32:20,662 - INFO - Processed 13200/16747 files for SENSEX\n",
      "2025-05-30 14:32:23,539 - INFO - Processed 13300/16747 files for SENSEX\n",
      "2025-05-30 14:32:26,549 - INFO - Processed 13400/16747 files for SENSEX\n",
      "2025-05-30 14:32:30,384 - INFO - Processed 13500/16747 files for SENSEX\n",
      "2025-05-30 14:32:32,958 - INFO - Processed 13600/16747 files for SENSEX\n",
      "2025-05-30 14:32:35,941 - INFO - Processed 13700/16747 files for SENSEX\n",
      "2025-05-30 14:32:39,590 - INFO - Processed 13800/16747 files for SENSEX\n",
      "2025-05-30 14:32:42,301 - INFO - Processed 13900/16747 files for SENSEX\n",
      "2025-05-30 14:32:45,344 - INFO - Processed 14000/16747 files for SENSEX\n",
      "2025-05-30 14:32:48,824 - INFO - Processed 14100/16747 files for SENSEX\n",
      "2025-05-30 14:32:52,106 - INFO - Processed 14200/16747 files for SENSEX\n",
      "2025-05-30 14:32:56,390 - INFO - Processed 14300/16747 files for SENSEX\n",
      "2025-05-30 14:32:59,867 - INFO - Processed 14400/16747 files for SENSEX\n",
      "2025-05-30 14:33:04,118 - INFO - Processed 14500/16747 files for SENSEX\n",
      "2025-05-30 14:33:07,645 - INFO - Processed 14600/16747 files for SENSEX\n",
      "2025-05-30 14:33:11,335 - INFO - Processed 14700/16747 files for SENSEX\n",
      "2025-05-30 14:33:14,766 - INFO - Processed 14800/16747 files for SENSEX\n",
      "2025-05-30 14:33:18,144 - INFO - Processed 14900/16747 files for SENSEX\n",
      "2025-05-30 14:33:21,991 - INFO - Processed 15000/16747 files for SENSEX\n",
      "2025-05-30 14:33:25,032 - INFO - Processed 15100/16747 files for SENSEX\n",
      "2025-05-30 14:33:28,293 - INFO - Processed 15200/16747 files for SENSEX\n",
      "2025-05-30 14:33:31,491 - INFO - Processed 15300/16747 files for SENSEX\n",
      "2025-05-30 14:33:35,845 - INFO - Processed 15400/16747 files for SENSEX\n",
      "2025-05-30 14:33:39,964 - INFO - Processed 15500/16747 files for SENSEX\n",
      "2025-05-30 14:33:43,867 - INFO - Processed 15600/16747 files for SENSEX\n",
      "2025-05-30 14:33:48,117 - INFO - Processed 15700/16747 files for SENSEX\n",
      "2025-05-30 14:33:51,393 - INFO - Processed 15800/16747 files for SENSEX\n",
      "2025-05-30 14:33:55,644 - INFO - Processed 15900/16747 files for SENSEX\n",
      "2025-05-30 14:33:58,577 - INFO - Processed 16000/16747 files for SENSEX\n",
      "2025-05-30 14:34:03,020 - INFO - Processed 16100/16747 files for SENSEX\n",
      "2025-05-30 14:34:06,132 - INFO - Processed 16200/16747 files for SENSEX\n",
      "2025-05-30 14:34:10,019 - INFO - Processed 16300/16747 files for SENSEX\n",
      "2025-05-30 14:34:13,079 - INFO - Processed 16400/16747 files for SENSEX\n",
      "2025-05-30 14:34:16,503 - INFO - Processed 16500/16747 files for SENSEX\n",
      "2025-05-30 14:34:20,026 - INFO - Processed 16600/16747 files for SENSEX\n",
      "2025-05-30 14:34:23,770 - INFO - Processed 16700/16747 files for SENSEX\n",
      "2025-05-30 14:34:27,560 - INFO - Combining 48 dataframe chunks for SENSEX\n",
      "2025-05-30 14:34:30,506 - INFO - Sorting 18782051 rows by timestamp for SENSEX\n",
      "2025-05-30 14:35:14,468 - INFO - Saving master parquet file: master_parquet_files/sensex_options_master.parquet\n",
      "2025-05-30 14:35:29,791 - INFO - ✅ Successfully created master parquet for SENSEX:\n",
      "2025-05-30 14:35:29,795 - INFO -    - Output file: master_parquet_files/sensex_options_master.parquet\n",
      "2025-05-30 14:35:29,796 - INFO -    - Total rows: 18,782,051\n",
      "2025-05-30 14:35:29,798 - INFO -    - Processed files: 16747/16747\n",
      "2025-05-30 14:35:29,799 - INFO -    - Failed files: 0\n",
      "2025-05-30 14:35:29,815 - INFO -    - Output size: 328.45 MB\n",
      "2025-05-30 14:35:29,817 - INFO -    - Processing time: 457.42 seconds\n",
      "2025-05-30 14:35:29,818 - INFO -    - Processing rate: 41061 rows/second\n",
      "2025-05-30 14:35:29,987 - INFO -    - Date range: 2023-10-20 09:15:00 to 2025-04-09 15:30:00\n",
      "2025-05-30 14:35:32,938 - INFO -    - Unique symbols: 4898\n",
      "2025-05-30 14:35:32,940 - INFO -    - Unique expiries: 96\n",
      "2025-05-30 14:35:32,942 - INFO -    - Unique strikes: 294\n",
      "2025-05-30 14:35:33,777 - INFO - ✅ Successfully completed SENSEX\n",
      "2025-05-30 14:35:33,779 - INFO - Generating final summary of created master parquet files\n",
      "2025-05-30 14:35:33,780 - INFO - Summary of 6 created master parquet files:\n",
      "2025-05-30 14:35:33,783 - INFO -   - BANKEX: 1,061,116 rows (15.69 MB)\n",
      "2025-05-30 14:35:33,826 - INFO -   - BANKNIFTY: 140,817,682 rows (2.56 GB)\n",
      "2025-05-30 14:35:33,843 - INFO -   - FINNIFTY: 21,422,448 rows (320.86 MB)\n",
      "2025-05-30 14:35:33,852 - INFO -   - MIDCPNIFTY: 16,192,878 rows (226.72 MB)\n",
      "2025-05-30 14:35:33,904 - INFO -   - NIFTY: 177,293,506 rows (2.90 GB)\n",
      "2025-05-30 14:35:33,926 - INFO -   - SENSEX: 18,782,051 rows (328.45 MB)\n",
      "2025-05-30 14:35:33,927 - INFO - Total size of all master files: 6.33 GB\n",
      "2025-05-30 14:35:33,929 - INFO - ======================================================================\n",
      "2025-05-30 14:35:33,929 - INFO - 🎉 Script completed in 477.74 seconds!\n",
      "2025-05-30 14:35:33,931 - INFO - ✅ Successfully processed: 2 underlyings\n",
      "2025-05-30 14:35:33,932 - INFO -    Success: BANKEX, SENSEX\n",
      "2025-05-30 14:35:33,933 - INFO - ❌ Failed to process: 0 underlyings\n",
      "2025-05-30 14:35:33,933 - INFO - 📊 Success rate: 100.0%\n",
      "2025-05-30 14:35:33,934 - INFO - ======================================================================\n"
     ]
    }
   ],
   "source": [
    "script_start_time = time.time()\n",
    "logger.info(\"=\" * 70)\n",
    "logger.info(\"Starting Options Master Parquet File Generation\")\n",
    "logger.info(\"=\" * 70)\n",
    "\n",
    "try:\n",
    "    ensure_output_directory()\n",
    "    \n",
    "    underlyings = find_available_underlyings()\n",
    "    \n",
    "    if not underlyings:\n",
    "        logger.error(\"No underlyings found. Exiting.\")\n",
    "    \n",
    "    successful_underlyings = []\n",
    "    failed_underlyings = []\n",
    "    \n",
    "    for underlying in underlyings:\n",
    "        logger.info(f\"Starting processing for underlying: {underlying}\")\n",
    "        \n",
    "        if create_master_parquet_for_underlying(underlying):\n",
    "            successful_underlyings.append(underlying)\n",
    "            logger.info(f\"✅ Successfully completed {underlying}\")\n",
    "        else:\n",
    "            failed_underlyings.append(underlying)\n",
    "            logger.error(f\"❌ Failed to process {underlying}\")\n",
    "    \n",
    "    generate_final_summary()\n",
    "    \n",
    "    script_elapsed_time = time.time() - script_start_time\n",
    "    logger.info(\"=\" * 70)\n",
    "    logger.info(f\"🎉 Script completed in {script_elapsed_time:.2f} seconds!\")\n",
    "    logger.info(f\"✅ Successfully processed: {len(successful_underlyings)} underlyings\")\n",
    "    \n",
    "    if successful_underlyings:\n",
    "        logger.info(f\"   Success: {', '.join(successful_underlyings)}\")\n",
    "    \n",
    "    logger.info(f\"❌ Failed to process: {len(failed_underlyings)} underlyings\")\n",
    "    \n",
    "    if failed_underlyings:\n",
    "        logger.info(f\"   Failed: {', '.join(failed_underlyings)}\")\n",
    "    \n",
    "    if len(successful_underlyings) + len(failed_underlyings) > 0:\n",
    "        success_rate = len(successful_underlyings) / (len(successful_underlyings) + len(failed_underlyings)) * 100\n",
    "        logger.info(f\"📊 Success rate: {success_rate:.1f}%\")\n",
    "    \n",
    "    logger.info(\"=\" * 70)\n",
    "    \n",
    "except Exception as e:\n",
    "    script_elapsed_time = time.time() - script_start_time\n",
    "    logger.error(f\"Script failed after {script_elapsed_time:.2f} seconds: {e}\")\n",
    "    raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab9e1ed3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
